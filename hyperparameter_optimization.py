from modules.make_datas import board_datas as bd
from network import DeepConvNet
from modules.common.trainer import Trainer
import numpy as np
from modules.common.util import time, Alerm
from modules.plot import plot
import pickle, os

pkl_file = None
pkl_file = "hyperparameter_optimization_info" + ".pkl"
if pkl_file != None:
    with open(pkl_file, 'rb') as f:
        r = pickle.load(f)
    plot.many_accuracy_graphs(r["results_train"], r["results_val"], graph_draw_num=20, col_num=5)
    plot.many_loss_graphs(r["results_losses"], graph_draw_num=20, col_num=5, verbose=False, smooth=False)
    plot.many_loss_graphs(r["results_losses"], graph_draw_num=20, col_num=5, verbose=False, smooth=True)
    exit()

# í•™ìŠµí•  ë°ì´í„° ë§Œë“¤ê¸°
x_datas, t_datas = bd.make_4to5()
(x_train, t_train), _ = bd.split_train_test(x_datas, t_datas)
(x_train, t_train), (x_val, t_val) = bd.split_train_val(x_train, t_train)
print(f"\ní›ˆë ¨ ë°ì´í„°, ê²€ì¦ ë°ì´í„°: {len(x_train)}ê°œ, {len(x_val)}ê°œ")

# ì‹œí—˜í•  ìµœì í™” ë°©ë²• ì¢…ë¥˜ì™€ í•™ìŠµë¥ , ê°€ì¤‘ì¹˜ ê°ì†Œ ê³„ìˆ˜ ë²”ìœ„ ì§€ì • (log ìŠ¤ì¼€ì¼ 10^n)
optimizers = {
    # 'SGD':      {"lr_min": -3, "lr_max": -1, "wd_min": -8, "wd_max": -4}
    # 'Momentum': {"lr_min": -3, "lr_max": -1, "wd_min": -8, "wd_max": -4}
    # 'Adagrad':  {"lr_min": -1, "lr_max": -5, "wd_min": -8, "wd_max": -4}
    'Adam':     {"lr_min": -1, "lr_max": -5, "wd_min": -8, "wd_max": -4}
    # 'Nesterov': {"lr_min": -3, "lr_max": -1, "wd_min": -8, "wd_max": -4}
    # 'Rmsprop':  {"lr_min": -3, "lr_max": -1, "wd_min": -8, "wd_max": -4}
}
optimization_trial = 50
attempts_number = 1
epochs = 10

give_up = {'epoch': 3} # 'test_acc':0.1
mini_batch_size = 5
results_losses = {}
results_train = {}
results_val = {}
digit = 6

# í•™ìŠµ ì •ë³´ ì¶œë ¥ í•¨ìˆ˜ ì •ì˜
def print_learning_info(str_lr, str_wd, val_accs, attempts_number, isgiveup):
    
    average_acc = sum(val_accs) / len(val_accs)          
    
    accuracy_info = "acc: None"
    if not isgiveup:
        accuracy_info = ("acc: %.2f%%" % average_acc).rjust(5)
    
    str_margin_of_error95 = ""
    if attempts_number > 1:
        standard_deviation = (sum( (np.array(val_accs)-average_acc)**2 ) / len(val_accs)) ** (1/2) ### ** ìš°ì„ ë„ê°€ /ë³´ë‹¤ ë†’ìŒ
        standard_error = standard_deviation / (len(val_accs) ** (1/2)) 
        margin_of_error95 = round(1.96 * standard_error, 2)

        str_margin_of_error95 = ("(Â±%.2f%%)" % margin_of_error95).rjust(5)
    
    print(f"lr: {str_lr.rjust(digit+2)} | wdâ: {str_wd} | {accuracy_info} {str_margin_of_error95}", end=" ")
    
    if isgiveup:
        print(f"-- I gave up")
    elif average_acc > 80:
        print("-- good!! ğŸ’š")
    else:
        print()

print(f"trial:{optimization_trial}, attempts:{attempts_number}, epochs:{epochs}")
print("\níƒìƒ‰ ì‹œì‘!!")
start_time = time.time()
first_trial = True

# ë§¤ê°œë³€ìˆ˜ ìµœì í™” ë°©ë²•ê³¼ í•™ìŠµë¥ ì— ë”°ë¥¸ ë”¥ëŸ¬ë‹ íš¨ìœ¨ ë¹„êµ
for optimizer in optimizers:

    for i in range(optimization_trial):
        lr = 10 ** np.random.uniform(optimizers[optimizer]["lr_min"], optimizers[optimizer]["lr_max"])
        wd = 10 ** np.random.uniform(optimizers[optimizer]["wd_min"], optimizers[optimizer]["wd_max"])

        val_accs = []
        for attempts in range(attempts_number):
            network = DeepConvNet(mini_batch_size=mini_batch_size, weight_decay_lambda=wd)
            
            trainer = Trainer(epochs=epochs, optimizer=optimizer, optimizer_param={'lr':lr}, verbose=False, 
                mini_batch_size=mini_batch_size, give_up=give_up, verbose_epoch=False,
                network=network, x_train=x_train, t_train=t_train, x_test=x_val, t_test=t_val)
            trainer.train()
            
            if trainer.isgiveup: break

            str_lr = f"%.{digit}f"% lr ### %{digit}.f
            str_wd = str(np.format_float_scientific(wd, precision=2)) # 
            key = (f"{optimizer} | " if len(optimizers) > 1 else "") + f"lr: {str_lr} | wd: {str_wd}" + (f" | {attempts}" if attempts > 1 else "")
            
            results_losses[key] = trainer.train_losses
            results_train[key] = trainer.train_accs ### *100 í•˜ë©´ 100ë²ˆ ë³µì‚¬ë¨
            results_val[key] = trainer.test_accs
            val_accs.append(trainer.test_accs[-1])
        
        if first_trial:
            first_trial = False
            first_trial_time = int(time.time() - start_time)
            estimated_time = first_trial_time * optimization_trial * attempts_number * len(optimizers)
            print(f"ì˜ˆìƒ ì†Œìš” ì‹œê°„: {time.str_hms(estimated_time)}")
        if i == 0: 
            print("\noptimizer: " + optimizer)
        print(f"{i+1}".rjust(2)+"/"+f"{optimization_trial}".rjust(2), end=" | ")
        print_learning_info(str_lr, str_wd, val_accs, attempts_number, isgiveup=trainer.isgiveup)

# í•™ìŠµ ëë‚˜ë©´ ì†Œìš” ì‹œê°„ ì¶œë ¥í•˜ê³ , ì •ë³´ ì €ì¥ í›„, ì•ŒëŒ ìš¸ë¦¬ê¸°
print(time.str_hms_delta(start_time))
Alerm()

def save_compare_optim_lr_info(results_train, results_val, results_losses, network_dir=network.network_dir, pkl_dir="saved_pkls"):
    results = {"results_train":results_train, "results_val":results_val, "results_losses":results_losses}
    
    pkl_file = f"hyperparameter_optimization_info".replace(":","").replace("'","").replace('"','')
    
    answer = input(f"\nì €ì¥í•  ìœ„ì¹˜: {pkl_dir}/(any) | í˜„ì¬ ìœ„ì¹˜(1) | ì €ì¥ ì•ˆí•¨(2): ")
    if answer == "1":
        pkl_path = f"{pkl_file}.pkl"
    elif answer == "2":
        print("ì €ì¥ ì•ˆí–ˆë‹¤^^")
        return
    else:
        pkl_path = f"{pkl_dir}/{network_dir}/{pkl_file}.pkl"
        
    if not os.path.exists(f"{pkl_dir}/{network_dir}"):
        os.makedirs(f"{pkl_dir}/{network_dir}")
        print(f"{pkl_dir}/{network_dir} í´ë” ìƒì„±")
    
    with open(pkl_path, 'wb') as f:
        pickle.dump(results, f)

    print(f"{pkl_file} ì €ì¥ ì„±ê³µ!")

save_compare_optim_lr_info(results_train, results_val, results_losses, network_dir=network.network_dir)

print("=========== Hyper-Parameter Optimization Result ===========")
plot.many_accuracy_graphs(results_train, results_val, graph_draw_num=20, col_num=5, sort=True)
plot.many_loss_graphs(results_losses, graph_draw_num=20, col_num=5, sort=True, verbose=False, smooth=False)
plot.many_loss_graphs(results_losses, graph_draw_num=20, col_num=5, sort=True, verbose=False, smooth=False)
